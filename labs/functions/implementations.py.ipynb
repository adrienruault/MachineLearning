{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from algorithms import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tested\n",
    "def least_squares_GD(y, tx, initial_w, max_iters, gamma):\n",
    "\n",
    "    # number of training data\n",
    "    N = tx.shape[0]\n",
    "    \n",
    "    # Define initial values of w and its associated mse loss\n",
    "    w_k = initial_w\n",
    "    loss_k = (1 / (2*N)) * np.linalg.norm(y - tx.dot(w_k))**2\n",
    "    \n",
    "    # stopping criterion definition:\n",
    "    n_iter = 0;\n",
    "    while (n_iter < max_iters):\n",
    "        # computation of the gradient\n",
    "        grad = -(1/N) * np.transpose(tx).dot(y - tx.dot(w_k))\n",
    "        \n",
    "        # update w\n",
    "        w_kp1 = w_k - gamma * grad\n",
    "        \n",
    "        # upsate loss wrt mse cost function\n",
    "        loss_kp1 = (1 / (2*N)) * np.linalg.norm(y - tx.dot(w_kp1))**2\n",
    "        \n",
    "        loss_k = loss_kp1\n",
    "        w_k = w_kp1\n",
    "        \n",
    "        # update n_iter: number of iterations\n",
    "        n_iter += 1\n",
    "        \n",
    "    # Printing the results\n",
    "    try:\n",
    "        initial_w.shape[1]\n",
    "        print(\"least_squares_GD({bi}/{ti}): loss={l}, w = {w}\".format(\n",
    "              bi=n_iter-1, ti=max_iters - 1, l=loss_k, w = w_k[:,0]))\n",
    "    except (IndexError, AttributeError):\n",
    "        print(\"least_squares_GD({bi}/{ti}): loss={l}, w = {w}\".format(\n",
    "                  bi=n_iter-1, ti=max_iters - 1, l=loss_k, w = w_k))\n",
    "    return w_k, loss_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tested\n",
    "def least_squares_SGD(y, tx, initial_w, max_iters, gamma):\n",
    "    # number of training data\n",
    "    N = tx.shape[0]\n",
    "    \n",
    "    # Define initial values of w and its associated mse loss\n",
    "    w_k = initial_w\n",
    "    loss_k = (1 / (2*N)) * np.linalg.norm(y - tx.dot(w_k))**2\n",
    "    \n",
    "    # stopping criterion definition:\n",
    "    n_iter = 0;\n",
    "    while (n_iter < max_iters):\n",
    "        # computation of the searching direction by sampling one training data from the data set\n",
    "        for y_b, tx_b in batch_iter(y, tx, 1):\n",
    "            g = - np.transpose(tx_b).dot(y_b - tx_b.dot(w_k))\n",
    "        \n",
    "        # update w_kp1\n",
    "        w_kp1 = w_k - gamma * g\n",
    "        \n",
    "        # upsate loss wrt mse cost function\n",
    "        loss_kp1 = (1 / (2*N)) * np.linalg.norm(y - tx.dot(w_kp1))**2\n",
    "        \n",
    "        loss_k = loss_kp1\n",
    "        w_k = w_kp1\n",
    "        \n",
    "        # update n_iter: number of iterations\n",
    "        n_iter += 1\n",
    "        \n",
    "    # Printing the results\n",
    "    try:\n",
    "        initial_w.shape[1]\n",
    "        print(\"least_squares_SGD({bi}/{ti}): loss={l}, w = {w}\".format(\n",
    "              bi=n_iter-1, ti=max_iters - 1, l=loss_k, w = w_k[:,0]))\n",
    "    except (IndexError, AttributeError):\n",
    "        print(\"least_squares_SGD({bi}/{ti}): loss={l}, w = {w}\".format(\n",
    "                  bi=n_iter-1, ti=max_iters - 1, l=loss_k, w = w_k))\n",
    "        \n",
    "    return w_k, loss_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tested\n",
    "def least_squares(y, tx):\n",
    "    N = tx.shape[0]\n",
    "    w = np.linalg.solve(np.transpose(tx).dot(tx), np.transpose(tx).dot(y))\n",
    "    loss = (1 / (2*N)) * np.linalg.norm(y - tx.dot(w))**2\n",
    "    \n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tested\n",
    "def ridge_regression(y, tx, lambda_):\n",
    "    N = tx.shape[0]\n",
    "    D = tx.shape[1]\n",
    "    lambda_prime = lambda_ * 2 * N\n",
    "    w = np.linalg.solve(np.transpose(tx).dot(tx) + lambda_prime * np.identity(D), np.transpose(tx).dot(y))\n",
    "    loss = (1 / (2*N)) * np.linalg.norm(y - tx.dot(w))**2 + lambda_ * np.linalg.norm(w)**2\n",
    "    return w, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\"\"\"\n",
    "    return np.exp(t) / (1 + np.exp(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tested\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \n",
    "    # check that initial_w has the wanted dimensions\n",
    "    try:\n",
    "        initial_w.shape[1]\n",
    "    except IndexError:\n",
    "        initial_w = np.expand_dims(initial_w, 1)\n",
    "    \n",
    "    # number of training data\n",
    "    N = tx.shape[0]\n",
    "    \n",
    "    # Define initial values of w and its associated mse loss\n",
    "    w_k = initial_w\n",
    "    loss_k = np.sum(np.log(1 + np.exp(tx.dot(w_k))) - y * tx.dot(w_k))\n",
    "    \n",
    "    # stopping criterion definition:\n",
    "    n_iter = 0;\n",
    "    while (n_iter < max_iters):\n",
    "        \n",
    "        # computation of the gradient\n",
    "        grad = np.transpose(tx).dot(sigmoid(tx.dot(w_k)) - y)\n",
    "        \n",
    "        # update w\n",
    "        w_kp1 = w_k - gamma * grad\n",
    "        \n",
    "        # upsate loss wrt mse cost function\n",
    "        loss_kp1 = np.sum(np.log(1 + np.exp(tx.dot(w_k))) - y * tx.dot(w_k))\n",
    "        \n",
    "        loss_k = loss_kp1\n",
    "        w_k = w_kp1\n",
    "        \n",
    "        # update n_iter: number of iterations\n",
    "        n_iter += 1\n",
    "        \n",
    "    # Printing the results\n",
    "    try:\n",
    "        initial_w.shape[1]\n",
    "        print(\"logistic_GD({bi}/{ti}): loss={l}, w = {w}\".format(\n",
    "              bi=n_iter-1, ti=max_iters - 1, l=loss_k, w = w_k[:,0]))\n",
    "    except (IndexError, AttributeError):\n",
    "        print(\"logistic_GD({bi}/{ti}): loss={l}, w = {w}\".format(\n",
    "                  bi=n_iter-1, ti=max_iters - 1, l=loss_k, w = w_k))\n",
    "    return w_k, loss_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tested\n",
    "def reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma):\n",
    "    \n",
    "    # convert w to a numpy array if it is passed as a list\n",
    "    if (type(initial_w) != np.ndarray):\n",
    "        initial_w = np.array(initial_w)\n",
    "    \n",
    "    # check that initial_w has the wanted dimensions\n",
    "    try:\n",
    "        initial_w.shape[1]\n",
    "    except IndexError:\n",
    "        initial_w = np.expand_dims(initial_w, 1)\n",
    "        \n",
    "    \n",
    "    # number of training data\n",
    "    N = tx.shape[0]\n",
    "    \n",
    "    # Define initial values of w and its associated mse loss\n",
    "    w_k = initial_w\n",
    "    loss_k = np.sum(np.log(1 + np.exp(tx.dot(w_k))) - tx.dot(w_k) * y) + (lambda_ / 2) * np.linalg.norm(w_k)**2\n",
    "    \n",
    "    # stopping criterion definition:\n",
    "    n_iter = 0;\n",
    "    while (n_iter < max_iters):        \n",
    "        # computation of the gradient\n",
    "        grad = np.transpose(tx).dot(sigmoid(tx.dot(w_k)) - y) + lambda_ * w_k\n",
    "        \n",
    "        # update w\n",
    "        w_kp1 = w_k - gamma * grad\n",
    "        \n",
    "        # upsate loss wrt mse cost function\n",
    "        loss_kp1 = np.sum(np.log(1 + np.exp(tx.dot(w_k))) - tx.dot(w_k) * y) + (lambda_ / 2) * np.linalg.norm(w_k)**2\n",
    "        \n",
    "        loss_k = loss_kp1\n",
    "        w_k = w_kp1\n",
    "        \n",
    "        # update n_iter: number of iterations\n",
    "        n_iter += 1\n",
    "        \n",
    "    # Printing the results\n",
    "    try:\n",
    "        initial_w.shape[1]\n",
    "        print(\"reg_logistic_GD({bi}/{ti}): loss={l}, w = {w}\".format(\n",
    "              bi=n_iter-1, ti=max_iters - 1, l=loss_k, w = w_k[:,0]))\n",
    "    except (IndexError, AttributeError):\n",
    "        print(\"reg_logistic_GD({bi}/{ti}): loss={l}, w = {w}\".format(\n",
    "                  bi=n_iter-1, ti=max_iters - 1, l=loss_k, w = w_k))\n",
    "    return w_k, loss_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testing_ls_ridge():\n",
    "    height, weight, gender = load_data_from_ex02(sub_sample=False, add_outlier=False)\n",
    "    x, mean_x, std_x = standardize(height)\n",
    "    y, tx = build_model_data(x, weight)\n",
    "        \n",
    "    w0_grid_test = np.linspace(-100, 100, 100)\n",
    "    w1_grid_test = np.linspace(-100, 100, 100)\n",
    "    grid_loss, grid_w = grid_search(y, tx, w0_grid_test, w1_grid_test)\n",
    "    \n",
    "    initial_w = [0, 0]\n",
    "    gamma_GD = 0.7\n",
    "    gamma_GD_mae = 10\n",
    "    max_iters = 500\n",
    "    GD_w, GD_loss = gradient_descent(y, tx, initial_w, max_iters, gamma_GD, cost='mse', tol=1e-2, thresh_test_div=10)\n",
    "    GD_w_mae, GD_loss_mae = gradient_descent(y, tx, initial_w, max_iters, gamma_GD_mae, cost='mae', tol=1e-2, thresh_test_div=10)\n",
    "\n",
    "    gamma_SGD = 0.01\n",
    "    gamma_SGD_mae = 0.1\n",
    "    max_iters = 1000\n",
    "    batch_size = 1\n",
    "    SGD_w, SGD_loss = stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma_SGD, cost='mse', tol=1e-4, thresh_test_div=100)\n",
    "    SGD_w_mae, SGD_loss_mae = stochastic_gradient_descent(y, tx, initial_w, batch_size, max_iters, gamma_SGD_mae, cost='mae', tol=1e-2, thresh_test_div=100)\n",
    "    \n",
    "    \n",
    "    \n",
    "    gamma_ridge = 0.01\n",
    "    lambda_ridge = 0.3\n",
    "    max_iters = 10000\n",
    "    \n",
    "    ridge_w, ridge_loss = gradient_descent(y, tx, initial_w, max_iters, gamma_ridge, cost='ridge', lambda_ = lambda_ridge, tol=1e-8, thresh_test_div=10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    max_iters_test = 1000\n",
    "    gamma_test = 0.01\n",
    "    test_w_GD, test_loss_GD = least_squares_GD(y, tx, initial_w, max_iters_test, gamma_test)\n",
    "    test_w_SGD, test_loss_SGD = least_squares_SGD(y, tx, initial_w, max_iters_test, gamma_test)\n",
    "    test_w_ls, test_loss_ls = least_squares(y, tx)\n",
    "    test_w_ridge, test_loss_ridge = ridge_regression(y, tx, lambda_ridge)\n",
    "\n",
    "    print(\"grid_w:\", grid_w)\n",
    "    print(\"GD_w:\", GD_w)\n",
    "    print(\"GD_w_mae:\", GD_w_mae)\n",
    "    print(\"SGD_w:\", SGD_w)\n",
    "    print(\"SGD_w_mae\", SGD_w_mae)\n",
    "    print(\"ridge_w:\", ridge_w)\n",
    "    print(\"test_w_GD:\", test_w_GD)\n",
    "    print(\"test_w_SGD:\", test_w_SGD)\n",
    "    print(\"test_w_ls:\", test_w_ls)\n",
    "    print(\"test_w_ridge:\", test_w_ridge)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    return 0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD(6/499), cost: mse: loss=15.386020684743531, w=[ 73.27789262  13.47676442]\n",
      "GD(11/499), cost: mae: loss=4.4280692313059635, w=[ 73.172       13.33995363]\n",
      "SGD(389/999), cost: mse: loss=16.31177891921242, w=[ 72.02696226  12.983129  ]\n",
      "SGD(676/999), cost: mae: loss=9.983030937042374, w=[ 64.7          6.74438094]\n",
      "GD(696/9999), cost: ridge: loss=1056.7049475009694, w=[ 45.80810073   8.42470983]\n",
      "least_squares_GD(999/999): loss=15.38589304420346, w = [ 73.29075781  13.4791305 ]\n",
      "least_squares_SGD(999/999): loss=15.49966273645759, w = [ 72.83520484  13.34883729]\n",
      "grid_w: [73.737373737373758, 13.131313131313135]\n",
      "GD_w: [ 73.27789262  13.47676442]\n",
      "GD_w_mae: [ 73.172       13.33995363]\n",
      "SGD_w: [ 72.02696226  12.983129  ]\n",
      "SGD_w_mae [ 64.7          6.74438094]\n",
      "ridge_w: [ 45.80810073   8.42470983]\n",
      "test_w_GD: [ 73.29075781  13.4791305 ]\n",
      "test_w_SGD: [ 72.83520484  13.34883729]\n",
      "test_w_ls: [ 73.293922    13.47971243]\n",
      "test_w_ridge: [ 45.80870125   8.42482027]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = [0, 0]\n",
    "numar = np.array([0, 3])\n",
    "type(numar) == np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_logistic():\n",
    "    # load data.\n",
    "    height, weight, gender = load_data_from_ex02()\n",
    "\n",
    "    # build sampled x and y.\n",
    "    seed = 1\n",
    "    y = np.expand_dims(gender, axis=1)\n",
    "    X = np.c_[height.reshape(-1), weight.reshape(-1)]\n",
    "    y, X = sample_data(y, X, seed, size_samples=200)\n",
    "    x, mean_x, std_x = standardize(X)\n",
    "    \n",
    "    max_iters = 100000\n",
    "    gamma = 0.001\n",
    "\n",
    "    # build tx\n",
    "    tx = np.c_[np.ones((y.shape[0], 1)), x]\n",
    "\n",
    "    initial_w = np.zeros((tx.shape[1], 1))\n",
    "    \n",
    "    lambda_ = 0.3\n",
    "    \n",
    "    w_logistic, loss_logistic = gradient_descent(y, tx, initial_w, max_iters, gamma, cost='logistic',\\\n",
    "                                                 lambda_=0, tol=1e-15, thresh_test_div=10, update_gamma=False)\n",
    "    \n",
    "    w_test_log, loss_test_log = logistic_regression(y, tx, initial_w, max_iters, gamma)\n",
    "    \n",
    "    w_reg_log, loss_reg_log = gradient_descent(y, tx, initial_w, max_iters, gamma, cost='reg_logistic',\\\n",
    "                                                 lambda_=lambda_, tol=1e-15, thresh_test_div=10, update_gamma=False)\n",
    "    \n",
    "    w_test_reg_log, loss_test_reg_log = reg_logistic_regression(y, tx, lambda_, initial_w, max_iters, gamma)\n",
    "    \n",
    "    print(\"w_logistic:\", w_logistic[:,0])\n",
    "    print(\"w_test_log:\", w_test_log[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD(27164/99999), cost: logistic: loss=40.894825609907, w=[ 0.23840571  2.46366568 -6.99516202]\n",
      "logistic_GD(99999/99999): loss=40.894825609890574, w = [ 0.23840601  2.46367002 -6.9951691 ]\n",
      "GD(11980/99999), cost: reg_logistic: loss=46.47750249347819, w=[ 0.15959974  1.26814654 -5.03057715]\n",
      "logistic_GD(99999/99999): loss=46.47750249347092, w = [ 0.15959985  1.26814854 -5.0305801 ]\n",
      "w_logistic: [ 0.23840571  2.46366568 -6.99516202]\n",
      "w_test_log: [ 0.23840601  2.46367002 -6.9951691 ]\n"
     ]
    }
   ],
   "source": [
    "testing_logistic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
